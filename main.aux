\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\@writefile{toc}{\contentsline {section}{\numberline {1}Nomenclature}{1}{section.1}\protected@file@percent }
\citation{TverskyKahneman1992}
\citation{Barberis2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Empirical phenomenon of probability weighting.} Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \cite  [p.\nobreakspace  {}310, Fig. 1]{TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the cumulative probability used by a DO) and look up the corresponding value on the vertical axis of the dotted inverse-S curve (the cumulative decision weight used by a DM). Low cumulative probabilities (left) are exceeded by their corresponding cumulative decision weights, and for high cumulative probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:TK1992}{{1}{2}{{\bf Empirical phenomenon of probability weighting.} Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \cite [p.~310, Fig. 1]{TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the cumulative probability used by a DO) and look up the corresponding value on the vertical axis of the dotted inverse-S curve (the cumulative decision weight used by a DM). Low cumulative probabilities (left) are exceeded by their corresponding cumulative decision weights, and for high cumulative probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.\relax }{figure.caption.1}{}}
\newlabel{eq:SN}{{3}{2}{Nomenclature}{equation.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Consistent probability weighting as a difference between models}{2}{section.2}\protected@file@percent }
\newlabel{sec:ModelDiff}{{2}{2}{Consistent probability weighting as a difference between models}{section.2}{}}
\newlabel{sec:The_inverse}{{2.1}{3}{The inverse-S curve\seclabel {The_inverse}}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The inverse-S curve}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Tversky and Kahneman}{3}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq:correspondence}{{4}{3}{Tversky and Kahneman}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Mechanistic explanation of the inverse S}{3}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{eq:DecisionW}{{5}{3}{Mechanistic explanation of the inverse S}{equation.2.5}{}}
\newlabel{eq:p}{{6}{3}{Mechanistic explanation of the inverse S}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  Mapping PDFs.} Left: probability PDF (red), estimated by a DO; and decision-weight PDF (blue), estimated by a DM. The DO models $x$ with a best estimate for the scale (standard deviation) and assumes the true frequency distribution is the red line. The DM models $x$ with a greater scale (here 2 times greater, $\alpha =2$), and assumes the true frequency distribution is the blue line. Comparing the two curves, the DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events, indicated by vertical arrows. Right: the difference between assigned probabilities can also be expressed by directly plotting, for any value of $x$, the two different PDFs against one another. This corresponds to a non-linear distortion of the horizontal axis. The arrows on the left correspond to the same $x$-values as on the right. They therefore start and end at identical vertical positions as on the left. Because of the non-linear distortion of the horizontal axis, they are shifted to different locations horizontally.\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:probability_dists}{{2}{4}{{\bf Mapping PDFs.} Left: probability PDF (red), estimated by a DO; and decision-weight PDF (blue), estimated by a DM. The DO models $x$ with a best estimate for the scale (standard deviation) and assumes the true frequency distribution is the red line. The DM models $x$ with a greater scale (here 2 times greater, $\alpha =2$), and assumes the true frequency distribution is the blue line. Comparing the two curves, the DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events, indicated by vertical arrows. Right: the difference between assigned probabilities can also be expressed by directly plotting, for any value of $x$, the two different PDFs against one another. This corresponds to a non-linear distortion of the horizontal axis. The arrows on the left correspond to the same $x$-values as on the right. They therefore start and end at identical vertical positions as on the left. Because of the non-linear distortion of the horizontal axis, they are shifted to different locations horizontally.\relax }{figure.caption.2}{}}
\newlabel{eq:w_of_p}{{7}{4}{Mechanistic explanation of the inverse S}{equation.2.7}{}}
\citation{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Mapping CDFs.} Left: The DO assumes the observable $X$ follows Gaussian distribution $X \sim \mathcal  {N}(0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi _{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution, $X \sim \mathcal  {N}(0,3)$ depicted by $F_w$ (blue). Following the vertical arrows (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's. Right: the same CDFs as on the left but now plotted not against $x$ but against the CDF $F_p$. Trivially, the CDF $F_p$ plotted against itself is the diagonal; the CDF $F_w$ now displays the generic inverse-S shape known from prospect theory. The arrows start and end at the same vertical values as on the left. Because the horizontal axis is has been non-linearly stretched (as the argument changed from $x$ to $F_p$), their horizontal locations are shifted. \relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:TwoCDFs}{{3}{5}{{\bf Mapping CDFs.} Left: The DO assumes the observable $X$ follows Gaussian distribution $X \sim \ND (0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi _{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution, $X \sim \ND (0,3)$ depicted by $F_w$ (blue). Following the vertical arrows (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's. Right: the same CDFs as on the left but now plotted not against $x$ but against the CDF $F_p$. Trivially, the CDF $F_p$ plotted against itself is the diagonal; the CDF $F_w$ now displays the generic inverse-S shape known from prospect theory. The arrows start and end at the same vertical values as on the left. Because the horizontal axis is has been non-linearly stretched (as the argument changed from $x$ to $F_p$), their horizontal locations are shifted. \relax }{figure.caption.3}{}}
\newlabel{sec:A_mismatch}{{2.2}{5}{A mismatch between both scales and locations\seclabel {A_mismatch}}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}A mismatch between both scales and locations}{5}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Decision weight CDFs used by a DM vs. probability CDFs used by a DO, Gaussian distribution}.  Top left: Difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 1.64 (broader than DO).  Top right: Difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.  Bottom left: Differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.64 (broader than DO).  Bottom right: Fit to observations reported by \cite  {TverskyKahneman1992}. This is (Eq.\nobreakspace  {}\ref  {eq:correspondence}) with $\gamma =0.65$. Note the similarity to bottom left.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:CDF_weights}{{4}{6}{{\bf Decision weight CDFs used by a DM vs. probability CDFs used by a DO, Gaussian distribution}.\\ Top left: Difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 1.64 (broader than DO).\\ Top right: Difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.\\ Bottom left: Differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.64 (broader than DO).\\ Bottom right: Fit to observations reported by \cite {TverskyKahneman1992}. This is \eref {correspondence} with $\gamma =0.65$. Note the similarity to bottom left.\relax }{figure.caption.4}{}}
\citation{Sunstein2020}
\newlabel{sec:Different_shapes}{{2.3}{7}{Different shapes, and fat-tailed distributions\seclabel {Different_shapes}}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Different shapes, and fat-tailed distributions}{7}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Probability weighting for Student-t distributions, where the DM uses a different shape parameter (1) and a different location parameter (0) from those of the DO (2 and 0.2, respectively).\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:fat_tailed_CDF}{{5}{8}{Probability weighting for Student-t distributions, where the DM uses a different shape parameter (1) and a different location parameter (0) from those of the DO (2 and 0.2, respectively).\relax }{figure.caption.5}{}}
\newlabel{sec:Reasons_for}{{3}{8}{Reasons for different models\seclabel {Reasons_for}}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reasons for different models}{8}{section.3}\protected@file@percent }
\newlabel{sec:tricky}{{3.1}{8}{``Probability'' can mean different things \seclabel {tricky}}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}``Probability'' can mean different things }{8}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Frequency-in-an-ensemble interpretation of probability}{9}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Frequency-over-time interpretation of probability}{9}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Degree-of-belief interpretation of probability}{9}{section*.8}\protected@file@percent }
\newlabel{sec:condition2}{{3.2}{9}{Consistent differences between DO and DM \seclabel {condition2}}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Consistent differences between DO and DM }{9}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces This table assumes $T = 10000$ observed time intervals. To be read as follows (first line): for an event of asymptotic probability 0.1, the most likely count in 10000 trials is 1000. Assuming Poisson statistics, this comes with an estimation error of $\sqrt  {1000} = 32$ in the count and $32/10000 = 0.003$ in the probability, which is $0.003/0.1=3\%$ of the asymptotic probability.\relax }}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:errors}{{1}{10}{This table assumes $T = 10000$ observed time intervals. To be read as follows (first line): for an event of asymptotic probability 0.1, the most likely count in 10000 trials is 1000. Assuming Poisson statistics, this comes with an estimation error of $\sqrt {1000} = 32$ in the count and $32/10000 = 0.003$ in the probability, which is $0.003/0.1=3\%$ of the asymptotic probability.\relax }{table.caption.10}{}}
\citation{Peters2019b}
\newlabel{eq:Poisson}{{11}{11}{Estimation errors for probabilities}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PDFs and inverse-S curves arising when the DO assumes a Gaussian (scale 1, location 0, top line) or a Student-t distribution (shape 2, location 0, bottom line), and the DM uses decision weights according to (Eq.\nobreakspace  {}\ref  {eq:Poisson}) with $T=10$ observations. For the fat-tailed Student-t distribution the difference between $p$ and $w$ is more pronounced.\relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig:square_root_error}{{6}{11}{PDFs and inverse-S curves arising when the DO assumes a Gaussian (scale 1, location 0, top line) or a Student-t distribution (shape 2, location 0, bottom line), and the DM uses decision weights according to \eref {Poisson} with $T=10$ observations. For the fat-tailed Student-t distribution the difference between $p$ and $w$ is more pronounced.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Typical situations of DO and DM: ergodicity}{11}{subsubsection.3.2.1}\protected@file@percent }
\citation{TverskyKahneman1992}
\citation{TverskyFox1995}
\newlabel{sec:Fitting_the}{{4}{12}{Fitting the model to experimental results \seclabel {Fitting_the}}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Fitting the model to experimental results }{12}{section.4}\protected@file@percent }
\citation{LattimoreBakerWitte1992}
\citation{tversky1995risk}
\citation{TverskyKahneman1992}
\citation{TverskyFox1995}
\citation{LattimoreBakerWitte1992}
\citation{TverskyKahneman1992}
\citation{LattimoreBakerWitte1992}
\citation{TverskyKahneman1992}
\citation{Levenberg1944}
\citation{TverskyKahneman1992}
\citation{TverskyFox1995}
\citation{LattimoreBakerWitte1992}
\citation{TverskyKahneman1992}
\citation{LattimoreBakerWitte1992}
\citation{TverskyKahneman1992}
\citation{Levenberg1944}
\citation{WuGonzalez1996,Prelec1998}
\newlabel{eq:LattimoreFunction}{{13}{13}{Fitting the model to experimental results \seclabel {Fitting_the}}{equation.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Model fitting to experimental data from \cite  {TverskyKahneman1992} (left) and \cite  {TverskyFox1995} (right). Left) \cite  {LattimoreBakerWitte1992}: $\delta =0.67\tmspace  +\thinmuskip {.1667em}\left (SE = 0.04\right )$, $\gamma =0.58\tmspace  +\thinmuskip {.1667em}\left (\pm 0.03\right )$; Gaussian model: $\mu =0.38\tmspace  +\thinmuskip {.1667em}\left (\pm 0.06\right )$, $\sigma =1.60\tmspace  +\thinmuskip {.1667em}\left (\pm 0.10\right )$; Student's-t model: $\nu =1.27\tmspace  +\thinmuskip {.1667em}\left (\pm 0.28\right )$, $\mu =0.40\tmspace  +\thinmuskip {.1667em}\left (\pm 0.07\right )$; \cite  {TverskyKahneman1992} (Eq.\nobreakspace  {}\ref  {eq:correspondence}): $\gamma =0.60\tmspace  +\thinmuskip {.1667em}\left (\pm 0.02\right )$. Right) \cite  {LattimoreBakerWitte1992}: $\delta =0.77\tmspace  +\thinmuskip {.1667em}\left (\pm 0.01\right )$, $\gamma =0.69\tmspace  +\thinmuskip {.1667em}\left (\pm 0.01\right )$; Gaussian model: $\mu =0.22\tmspace  +\thinmuskip {.1667em}\left (\pm 0.01\right )$, $\sigma =1.41\tmspace  +\thinmuskip {.1667em}\left (\pm 0.03\right )$; Student's-t model: $\nu =1.41\tmspace  +\thinmuskip {.1667em}\left (\pm 0.21\right )$, $\mu =0.22\tmspace  +\thinmuskip {.1667em}\left (\pm 0.03\right )$; \cite  {TverskyKahneman1992} (Eq.\nobreakspace  {}\ref  {eq:correspondence}): $\gamma =0.68\tmspace  +\thinmuskip {.1667em}\left (\pm 0.01\right )$. Shaded areas indicate two standard errors in the fitted parameter values. The fit was done by implementing the Levenberg-Marquardt algorithm \cite  {Levenberg1944} for non-linear least squares curve fitting. \relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:TK_TF_fit}{{7}{13}{Model fitting to experimental data from \cite {TverskyKahneman1992} (left) and \cite {TverskyFox1995} (right). Left) \cite {LattimoreBakerWitte1992}: $\delta =0.67\,\left (SE = 0.04\right )$, $\gamma =0.58\,\left (\pm 0.03\right )$; Gaussian model: $\mu =0.38\,\left (\pm 0.06\right )$, $\sigma =1.60\,\left (\pm 0.10\right )$; Student's-t model: $\nu =1.27\,\left (\pm 0.28\right )$, $\mu =0.40\,\left (\pm 0.07\right )$; \cite {TverskyKahneman1992} \eref {correspondence}: $\gamma =0.60\,\left (\pm 0.02\right )$. Right) \cite {LattimoreBakerWitte1992}: $\delta =0.77\,\left (\pm 0.01\right )$, $\gamma =0.69\,\left (\pm 0.01\right )$; Gaussian model: $\mu =0.22\,\left (\pm 0.01\right )$, $\sigma =1.41\,\left (\pm 0.03\right )$; Student's-t model: $\nu =1.41\,\left (\pm 0.21\right )$, $\mu =0.22\,\left (\pm 0.03\right )$; \cite {TverskyKahneman1992} \eref {correspondence}: $\gamma =0.68\,\left (\pm 0.01\right )$. Shaded areas indicate two standard errors in the fitted parameter values. The fit was done by implementing the Levenberg-Marquardt algorithm \cite {Levenberg1944} for non-linear least squares curve fitting. \relax }{figure.caption.12}{}}
\citation{Stott2006}
\citation{Prelec1998}
\citation{Wakker2010}
\citation{AbdellaouiETAL2011}
\citation{Lopes1991,Gigerenzer2018}
\citation{Sunstein2020}
\bibstyle{apalike}
\bibdata{../LML_bibliography/bibliography}
\bibcite{AbdellaouiETAL2011}{{1}{2011}{{Abdellaoui et~al.}}{{}}}
\bibcite{Barberis2013}{{2}{2013}{{Barberis}}{{}}}
\bibcite{Gigerenzer2018}{{3}{2018}{{Gigerenzer}}{{}}}
\bibcite{LattimoreBakerWitte1992}{{4}{1992}{{Lattimore et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion/Summary}{14}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Related literature}{14}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results set in perspective}{14}{section*.14}\protected@file@percent }
\bibcite{Levenberg1944}{{5}{1944}{{Levenberg}}{{}}}
\bibcite{Lopes1991}{{6}{1991}{{Lopes}}{{}}}
\bibcite{Peters2019b}{{7}{2019}{{Peters}}{{}}}
\bibcite{Prelec1998}{{8}{1998}{{Prelec}}{{}}}
\bibcite{Stott2006}{{9}{2006}{{Stott}}{{}}}
\bibcite{Sunstein2020}{{10}{2020}{{Sunstein}}{{}}}
\bibcite{TverskyFox1995}{{11}{1995}{{Tversky and Fox}}{{}}}
\bibcite{TverskyKahneman1992}{{12}{1992}{{Tversky and Kahneman}}{{}}}
\bibcite{tversky1995risk}{{13}{1995}{{Tversky and Wakker}}{{}}}
\bibcite{Wakker2010}{{14}{2010}{{Wakker}}{{}}}
\bibcite{WuGonzalez1996}{{15}{1996}{{Wu and Gonzalez}}{{}}}
