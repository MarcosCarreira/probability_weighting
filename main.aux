\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\bibstyle{biblatex}
\bibdata{main-blx,../LML_bibliography/bibliography}
\citation{biblatex-control}
\abx@aux@refcontext{nyt/global//global/global}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{UKenglish}{}
\BKM@entry{id=1,dest={73656374696F6E2E31},srcline={395}}{4E6F6D656E636C617475726520616E64206B6579206F62736572766174696F6E}
\citation{Barberis2013}
\abx@aux@cite{Barberis2013}
\abx@aux@segm{0}{0}{Barberis2013}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\@writefile{toc}{\contentsline {section}{\numberline {1}Nomenclature and key observation}{2}{section.1}\protected@file@percent }
\abx@aux@backref{1}{Barberis2013}{0}{2}{2}
\abx@aux@page{1}{2}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\abx@aux@backref{2}{TverskyKahneman1992}{0}{3}{3}
\abx@aux@backref{3}{TverskyKahneman1992}{0}{3}{3}
\abx@aux@page{3}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \textcite {TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the probability used by a DM) and look up the corresponding value on the vertical axis (the decision weight). For low (cumulative) probabilities (left) the decision weight exceeds the probability, and for high probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.\relax }}{3}{figure.caption.1}\protected@file@percent }
\abx@aux@backref{6}{TverskyKahneman1992}{0}{3}{3}
\abx@aux@backref{7}{TverskyKahneman1992}{0}{3}{3}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:TK1992}{{1}{3}{Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \textcite {TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the probability used by a DM) and look up the corresponding value on the vertical axis (the decision weight). For low (cumulative) probabilities (left) the decision weight exceeds the probability, and for high probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.\relax }{figure.caption.1}{}}
\newlabel{eq:SN}{{1.3}{3}{Nomenclature and key observation}{equation.1.3}{}}
\BKM@entry{id=2,dest={73656374696F6E2E32},srcline={451}}{436F6E73697374656E742070726F626162696C69747920776569676874696E67206173206120646966666572656E6365206265747765656E206D6F64656C73}
\BKM@entry{id=3,dest={73756273656374696F6E2E322E31},srcline={456}}{47656E6572696320636173653A2074686520476175737369616E206578616D706C65}
\@writefile{toc}{\contentsline {section}{\numberline {2}Consistent probability weighting as a difference between models}{4}{section.2}\protected@file@percent }
\newlabel{sec:ModelDiff}{{2}{4}{Consistent probability weighting as a difference between models}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generic case: the Gaussian example}{4}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textsf  {\textbf  {Two PDFs.}} Probability density function (blue), estimated by a DO; and decision-weight density function (red), estimated by a DM. The DO models returns with a best estimate for the variance and assumes the true frequency distribution is the blue line. The DM wants to be on the safe side, models returns with a greater variance, and assumes the true frequency distribution is the red line. The DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events.\relax }}{5}{figure.caption.2}\protected@file@percent }
\newlabel{fig:probability_dists}{{2}{5}{\bsf {Two PDFs.} Probability density function (blue), estimated by a DO; and decision-weight density function (red), estimated by a DM. The DO models returns with a best estimate for the variance and assumes the true frequency distribution is the blue line. The DM wants to be on the safe side, models returns with a greater variance, and assumes the true frequency distribution is the red line. The DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events.\relax }{figure.caption.2}{}}
\newlabel{eq:DecisionW}{{2.1}{5}{Generic case: the Gaussian example}{equation.2.1}{}}
\newlabel{eq:p}{{2.2}{5}{Generic case: the Gaussian example}{equation.2.2}{}}
\newlabel{eq:q_of_p}{{2.3}{5}{Generic case: the Gaussian example}{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Decision weight density (used by a DM) vs. probability density (used by a DO) for the Gaussian model (blue), compared to the diagonal (black) where DM and DO use the same parameters. For low probabilities, the decision weights are higher than the probabilities; for high probabilities they are lower.\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:probability_weights}{{3}{6}{Decision weight density (used by a DM) vs. probability density (used by a DO) for the Gaussian model (blue), compared to the diagonal (black) where DM and DO use the same parameters. For low probabilities, the decision weights are higher than the probabilities; for high probabilities they are lower.\relax }{figure.caption.3}{}}
\BKM@entry{id=4,dest={73756273656374696F6E2E322E32},srcline={552}}{4C61636B206F6620636F6E6365707475616C20636C61726974792061626F75742077686F73652070726F626162696C69746965732061726520616E616C797365643F}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textsf  {\textbf  {Two Gaussian CDFs.}} The DO assumes the observable $X$ follows Gaussian distribution $X \sim \mathcal  {N}(0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi _{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution with identical location, \textit  {i.e.}\xspace  a Gaussian with larger scale, $X \sim \mathcal  {N}(0,3)$ depicted by $F_w$ (blue). Following the dashed vertical lines (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:TwoCDFs}{{4}{7}{\bsf {Two Gaussian CDFs.} The DO assumes the observable $X$ follows Gaussian distribution $X \sim \ND (0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi _{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution with identical location, \ie a Gaussian with larger scale, $X \sim \ND (0,3)$ depicted by $F_w$ (blue). Following the dashed vertical lines (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Lack of conceptual clarity about whose probabilities are analysed?}{7}{subsection.2.2}\protected@file@percent }
\newlabel{sec:tricky}{{2.2}{7}{Lack of conceptual clarity about whose probabilities are analysed?}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Frequency-in-an-ensemble interpretation of probability}{7}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Frequency-over-time interpretation of probability}{8}{section*.6}\protected@file@percent }
\BKM@entry{id=5,dest={73756273656374696F6E2E322E33},srcline={593}}{446973696E7465726573746564206F627365727665727320616E64206465636973696F6E206D616B6573206861766520646966666572656E742070657273706563746976657320}
\citation{Peters2019b}
\abx@aux@cite{Peters2019b}
\abx@aux@segm{0}{0}{Peters2019b}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces This table assumes $T = \num {10000}$ observed time intervals. To be read as follows (first line): for an event of true probability 0.5, the most likely count in $\num {10000}$ trials is $\num {5000}$. Assuming Poissonian statistics, this comes with an estimation error of $\sqrt  {\num {5000}}/\num {5000}=0.01$, which is 2\% of the true probability.\relax }}{9}{table.caption.8}\protected@file@percent }
\newlabel{tab:errors}{{1}{9}{This table assumes $T = \num {10000}$ observed time intervals. To be read as follows (first line): for an event of true probability 0.5, the most likely count in $\num {10000}$ trials is $\num {5000}$. Assuming Poissonian statistics, this comes with an estimation error of $\sqrt {\num {5000}}/\num {5000}=0.01$, which is 2\% of the true probability.\relax }{table.caption.8}{}}
\newlabel{sec:condition2}{{2.3}{9}{Disinterested observers and decision makes have different perspectives \seclabel {condition2}}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Disinterested observers and decision makes have different perspectives }{9}{subsection.2.3}\protected@file@percent }
\abx@aux@backref{8}{Peters2019b}{0}{9}{9}
\abx@aux@page{8}{9}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\abx@aux@backref{9}{TverskyKahneman1992}{0}{10}{10}
\abx@aux@backref{10}{TverskyKahneman1992}{0}{10}{10}
\abx@aux@page{10}{10}
\abx@aux@backref{11}{TverskyKahneman1992}{0}{10}{10}
\abx@aux@backref{12}{TverskyKahneman1992}{0}{10}{10}
\abx@aux@page{12}{10}
\newlabel{eq:correspondence}{{2.4}{10}{Disinterested observers and decision makes have different perspectives \seclabel {condition2}}{equation.2.4}{}}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\citation{TverskyKahneman1992}
\abx@aux@cite{TverskyKahneman1992}
\abx@aux@segm{0}{0}{TverskyKahneman1992}
\BKM@entry{id=6,dest={73656374696F6E2E33},srcline={651}}{4F746865722070726F626162696C69747920646973747269627574696F6E73}
\abx@aux@backref{13}{TverskyKahneman1992}{0}{11}{11}
\abx@aux@page{13}{11}
\@writefile{toc}{\contentsline {section}{\numberline {3}Other probability distributions}{11}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Decision weight CDFs used by a DM vs. probability CDFs used by a DO.  Top left) Gaussian distribution, difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 2.7 (broader than DO).  Top right) Gaussian distribution, difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1. Bottom left) Gaussian distribution, differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 2.7 (broader than DO).  Bottom right) Fit to observations reported by \textcite {TverskyKahneman1992}. This is Eq.~(\ref  {eq:correspondence}) with $\alpha =0.65$. The observations by \textcite {TverskyKahneman1992} are consistent with a DM assuming a scale and location in real-world decisions that differ from those assumed by the DO.\relax }}{12}{figure.caption.9}\protected@file@percent }
\abx@aux@backref{18}{TverskyKahneman1992}{0}{12}{12}
\abx@aux@backref{19}{TverskyKahneman1992}{0}{12}{12}
\abx@aux@backref{20}{TverskyKahneman1992}{0}{12}{12}
\abx@aux@backref{21}{TverskyKahneman1992}{0}{12}{12}
\newlabel{fig:CDF_weights}{{5}{12}{Decision weight CDFs used by a DM vs. probability CDFs used by a DO.\\ Top left) Gaussian distribution, difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 2.7 (broader than DO).\\ Top right) Gaussian distribution, difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1. Bottom left) Gaussian distribution, differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 2.7 (broader than DO).\\ Bottom right) Fit to observations reported by \textcite {TverskyKahneman1992}. This is \eref {correspondence} with $\alpha =0.65$. The observations by \textcite {TverskyKahneman1992} are consistent with a DM assuming a scale and location in real-world decisions that differ from those assumed by the DO.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Probability weighting for Student-t distributions, where the DM uses a different shape parameter (1) and a different location parameter (0) from those of the DO (2 and 0.2, respectively).\relax }}{13}{figure.caption.10}\protected@file@percent }
\newlabel{fig:other_CDFs}{{6}{13}{Probability weighting for Student-t distributions, where the DM uses a different shape parameter (1) and a different location parameter (0) from those of the DO (2 and 0.2, respectively).\relax }{figure.caption.10}{}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Barberis2013}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Peters2019b}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{TverskyKahneman1992}{nyt/global//global/global}
\abx@aux@defaultlabelprefix{0}{Barberis2013}{}
\abx@aux@defaultlabelprefix{0}{Peters2019b}{}
\abx@aux@defaultlabelprefix{0}{TverskyKahneman1992}{}
\abx@aux@page{22}{14}
\abx@aux@page{23}{14}
\abx@aux@page{24}{14}
