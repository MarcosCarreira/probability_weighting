\documentclass[a4paper, 12pt]{article}

\usepackage[sort&compress]{natbib}
\bibpunct{(}{)}{;}{a}{}{,} 

\usepackage{amsthm, amsmath, amssymb, mathrsfs, multirow, url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{makecell,booktabs,siunitx}
\usepackage{graphicx} 
\usepackage{ifthen} 
\usepackage{amsfonts}
\usepackage[usenames]{color}
\usepackage{placeins}
%\usepackage{fullpage}

\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\scalebox{0.25}{
\includegraphics{resone-logo.png}}
%\includegraphics{R-dot-one.png}}
\rhead{
{\small 
\url{https://www.researchers.one/article/2019-03-4}} % fill in URL here
}
}

\usepackage{xspace}
%\usepackage{amsmath}
%\usepackage{natbib}
%\usepackage{comment}
%\usepackage{graphicx}
%\usepackage{authblk}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}


\newcommand{\ave}[1]{\left\langle#1 \right\rangle}
\newcommand{\gamble}[1]{{\sc#1}}
\newcommand{\person}[1]{{\sc#1}}
\newcommand{\concept}[1]{{\sc#1}}
\newcommand{\elabel}[1]{\label{eq:#1}}
\newcommand{\eref}[1]{(Eq.~\ref{eq:#1})}
\newcommand{\ceref}[2]{(\ref{eq:#1}#2)}
\newcommand{\Eref}[1]{Equation~(\ref{eq:#1})}
\newcommand{\flabel}[1]{\label{fig:#1}}
\newcommand{\fref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Fref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tlabel}[1]{\label{tab:#1}}
\newcommand{\tref}[1]{Tab.~\ref{tab:#1}}
\newcommand{\Tref}[1]{Table~\ref{tab:#1}}
\newcommand{\seclabel}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Sec.~\ref{sec:#1}}
\newcommand{\Secref}[1]{Section~\ref{sec:#1}}
\newcommand{\nmax}{{n_{\text{max}}}\xspace}
\newcommand{\gens}{g_\text{e}}
\newcommand{\gtime}{\bar{g}}
\newcommand{\gi}{g^{(i)}}
\newcommand{\Ito}{It\^{o}\xspace}
\newcommand{\etal}{{\it et~al.}\xspace}
\newcommand{\apriori}{{\it a priori}\xspace}
\newcommand{\ie}{{\it i.e.}\ }
\newcommand{\etc}{{\it etc.}\xspace}
\newcommand{\eg}{{\it e.g.}\ }
\newcommand{\cf}{{\it c.f.}\ }
\newcommand{\vv}{{\it v.v.}\ }
\newcommand{\gensst}{g_{\text{est}}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\D}{\Delta}
\newcommand{\yn}[1]{{y^{(#1)}}}
\newcommand{\xin}[1]{{\xi^{(#1)}}}
\newcommand{\Wn}[1]{{W^{(#1)}}}
\newcommand{\mun}[1]{{\mu^{(#1)}}}
\newcommand{\sigman}[1]{{\sigma^{(#1)}}}
\newcommand{\sigmac}{{\sigma_\rho}}

\newcommand{\ND}{\mathcal{N}} % Normal Distribution
\newcommand{\sigmat}{\tilde{\sigma}}
%\newcommand{\var}[1]{\text{var}(#1)}
\newcommand{\MK}[1]{{\it ***MK: #1 MK***}}
\newcommand{\OP}[1]{{\it ***OP: #1 OP***}}
\newcommand{\YB}[1]{{\it ***YB: #1 YB***}}


\title{What are we weighting for? A mechanistic model for probability weighting}
\author{
Ole Peters\footnote{London Mathematical Laboratory, 8 Margravine Gardens, London W6 8RH, UK and Santa Fe Institute, 1399 Hyde Park Road, Santa Fe, 87501 NM, USA. Email: \texttt{o.peters@lml.org.uk}} \;
Alexander Adamou\footnote{London Mathematical Laboratory, 8 Margravine Gardens, London W6 8RH, UK. Email: \texttt{a.adamou@lml.org.uk}} \;
Mark Kirstein\footnote{London Mathematical Laboratory, 8 Margravine Gardens, London W6 8RH, UK. Email: \texttt{m.kirstein@lml.org.uk}} \;
Yonatan Berman\footnote{London Mathematical Laboratory, 8 Margravine Gardens, London W6 8RH, UK. Email: \texttt{y.berman@lml.org.uk}} 
}
\date{\today}

\begin{document}
\begin{titlepage}
	\maketitle
\thispagestyle{fancy}

\begin{abstract}
\noindent 
Behavioural economics collects observations of human economic behaviour and provides labels for those observations. 
Probability weighting is one such label. It expresses a mismatch between probabilities used in a formal model of a decision problem (\ie model parameters) and probabilities inferred from real people's behaviour faced with the modelled decision problem (the same parameters estimated empirically). The inferred probabilities are called ``decision weights.'' 
It is considered a robust observation that decision weights are higher than probabilities for extreme events, and (necessarily, because of normalisation) lower than probabilities for common events.
The observed behaviour thus amounts to the refusal by real decision-makers totally to rely on a formal model, and instead to exercise extra caution. In this paper we explore quantitatively how such caution generically reproduces existing empirical findings. We list quantitatively  
well-specified reasons for such caution and find the resulting probability weighting, as a benchmark for reasonable behaviour.

\vspace{1em}

\noindent\textsf{\textbf{Keywords}} ~ Decision Theory, Prospect Theory, Probability Weighting, Ergodicity Economics
\vspace{.5em}

\noindent\textsf{\textbf{JEL Codes}} ~
% \href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#B}{%
% B16, % HET: Quantitative and Mathematical 
\href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#C}{%
% C60		% Mathematical Models
% $\cdot$
C61		% Optimization Techniques • Programming Models • Dynamic Analysis
$\cdot$
}%
\href{https://www.aeaweb.org/econlit/jelCodes.php?view=jel#D}{%
D01 	% Microeconomic Behaviour: Underlying Principles
$\cdot$
D81 	% Criteria for Decision-Making under Risk and Uncertainty
% $\cdot$
% D9		% Micro-Based Behavioral Economics 
}
\end{abstract}
\end{titlepage}

%\section{Introduction and key observation}
%In this paper, we provide a mechanistic explanation of the phenomenon traditionally labelled as probability weighting, which originates in prospect theory.\footnote{\cite{TverskyKahneman1992} For a useful survey see \cite{Barberis2013}.} We find it to be a beneficial behavioural pattern instead of a systematic irrational bias. 
 
%We find that the phenomenon is a reasonable way to cope with uncertainty in the real world of a specific kind.
 
%Some processes in nature have random outcomes. Model of such processes may therefore comprise random variables and stochastic processes. However, on top of the inherent stochasticity of nature captured in such a model, there is additional model uncertainty, often referred to as radical or fundamental uncertainty. In its mildest form, model uncertainty can manifest itself in parametric uncertainty within a class of models (\eg uncertainty in the estimates of the mean of a normal distribution). More severe forms are conceivable (\eg uncertainty about the correct class of models/probability distributions) but are not necessary for our mechanistic explanation of probability weighting.
% 
%We thereby conceptualise probability weighting as a reasonable pattern in human behaviour to cope with the fundamental uncertainty in the real world.
%\OP{We don't invoke radical uncertainty. Just regular uncertainty in parameter estimates is sufficient.}

%\paragraph{Organisation of the paper} The remainder is organised as follows \ldots
%We begin by discussing the necessity for using decision weights that are greater than the probabilities for rare events. Next, we discuss as an example how the same effect can arise when repeatedly facing prospects with a Gaussian random variable whose mean fluctuates in time.
%
%Finally, we calculate analytically, and plot, the probability weighting curves one would expect on this basis. We compare these to the empirical results summarised by \cite{Barberis2013}, and we comment on the function fitted to these data by \cite{TverskyKahneman1992}.
%
%Our work contributes to the growing field of ergodicity economics
%\cite{Peters2019b} where decision makers are modelled as behaving optimally over time (as opposed to optimally in a statistical-ensemble sense).


\section{Nomenclature}
{\it Probability weighting} is a concept that originated in prospect theory. It is one way to conceptualize a pattern in human behavior, of caution with respect to formal models.
%The key result of this paper is that probability weighting is a reasonable act of caution of a decision maker with respect to the additional fundamental uncertainty with regard to formal models of the random processes in the real world. 
%The results of this paper are best explained using the following example:  
This is best explained with an example:
\bi
	\item a \textit{disinterested observer} (DO), such as an experimenter, tells
	\item a \textit{decision maker} (DM) 
\ei
that an event occurs with some probability. The DM's behaviour is then observed, and is found to be consistent with a behavioural model (for example expected-utility optimization) where the DM uses a probability that differs systematically from what the DO has declared.
% 
Specifically, it is consistently observed that DMs act as though extreme events (those of low probability) had higher probabilities than what's specified by the DO. These apparent ``higher probabilities'' are called ``{\it decision weights}'' because they are better at describing the decisions actually made than the probabilities specified by the DO. We will adopt this nomenclature here. 
\bi
	\item By ``\textit{probabilities,}'' expressed as probability density functions (PDFs) and denoted $p(x)$, we will mean the numbers specified by a DO.
	\item By ``\textit{decision weights,}'' also expressed as PDFs and denoted $w(x)$, we will mean the numbers that best describe the behaviour of a DM.\footnote{In the literature, decision weights are not always normalised, but for simplicity we will work with normalised decision weights. Mathematically speaking, they are therefore proper probabilities even though we don't call them that. Our results are unaffected because normalising just means dividing by a constant (the sum or integral of the non-normalised decision weights).}
\ei

The key observation of $w(x)>p(x)$ for small $p$ \etc is often summarised visually with a comparison between 
\bi
\item cumulative density functions (CDFs) for probabilities, denoted 
\be
F_p(x)=\int_{-\infty}^x p(s) ds
\ee
\item and CDFs for decision weights, denoted
\be
F_w(x)=\int_{-\infty}^x w(s) ds ~.
\ee
\ei
In \fref{TK1992} we reproduce the first such visual summary from \cite[p.~310]{TverskyKahneman1992}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{./figs/TK1992.PNG}
\caption{{\bf Empirical phenomenon of probability weighting.} Cumulative decision weights $F_w$ (used by decision makers) versus cumulative probabilities $F_p$ (used by disinterested observers), as reported by \cite[p.~310, Fig. 1]{TverskyKahneman1992}. The figure is to be read as follows: pick a point along the horizontal axis (the cumulative probability used by a DO) and look up the corresponding value on the vertical axis of the dotted inverse-S curve (the cumulative decision weight used by a DM). Low cumulative probabilities (left) are exceeded by their corresponding cumulative decision weights, and for high cumulative probabilities it's the other way around. It's the inverse-S shape of the curve that indicates this qualitative relationship.}
\flabel{TK1992}
\end{figure}

As a final piece of nomenclature, we will use the terms \textit{location}, \textit{scale}, and {\it shape} when discussing probability distributions. Consider a standard normal distribution $\ND(0,1)$ -- here, the parameters indicate location 0 and squared scale 1 (for a Gaussian the location is the mean and scale is the standard deviation). For a general random variable $X$, with arbitrary parameters for location $\mu_X$ and scale $\sigma_X$, the transformation in \eref{SN} obtains the identically-shaped location-0 and scale-1 distribution for the so standardised random variable
\be \elabel{SN}
	Z = \frac{X-\overbrace{\mu_X \mathstrut}^{\text{location}}}{\underbrace{\sigma_X\mathstrut}_{\text{scale}}}
%		~, \qquad \qquad Z \sim \ND(0,1
~.
\ee
Thus the PDF of $Z$, $p(z)$ is a density with location $\mu_Z=0$ and scale $\sigma_Z=1$. In a graph of a distribution, a change of location shifts the curve to the left or right, and a change in scale shrinks or blows up the width of its features. Neither operation changes the {\it shape} of the distribution.

% We will show below that these observations are predicted by considerations of uncertainty about probabilities, or more generally by uncertainty about model parameters.


\section{Consistent probability weighting as a difference between models} \seclabel{ModelDiff}

Behavioural economics interprets \fref{TK1992} as evidence for a cognitive bias of the DM. We will keep a neutral stance. We don't assume the DO to know ``the truth'' -- he has a model of the world. Nor do we assume the DM to know ``the truth'' -- he has another model of the world. From our perspective \fref{TK1992} merely shows that the two models differ. It says nothing about who is right or wrong.

%Below we will establish what is needed to generate an inverse-S shape like in \fref{KT1992}. Next, we list mechanistic reasons 
%go through a few generic reasons why the models may differ, and we will find that the inverse-S curve is a robust prediction that arises when we emphasize that DMs have to operate along time lines. In this sense, it is a prediction that arises in ergodicity economics.

%\MK{Reference to EE falls from heaven, maybe refer to it in the intro. See TL \& discounting}

\subsection{The inverse-S curve\seclabel{The_inverse}}
\subsubsection{Tversky and Kahneman}
\citet{TverskyKahneman1992} chose to fit the empirical data in \fref{TK1992} with the following function\footnote{\Eref{correspondence} is the consensus functional form in the community \cite{Barberis2013}.}
% Whereas we provide a mechanistic explanation, psychological explanations prevail in the behavioural economics and finance literature, see \cite{WuGonzalez1996,Prelec1998,GonzalezWu1999,Stott2006,DeGiorgi2006,Wakker2010,AbdellaouiETAL2011} and references especially in the latter. It remains doubtful how a psychological explanation shall compensate for a missing and/or not yet understood mechanism, generating a particular phenomenon like probability weighting.
%\OP{I think I see the point, but I don't know what a ``psychological explanation'' is; it may also be unclear what we mean by ``mechanistic.''}
% 
\be
\elabel{correspondence}
F_w^*\left(F_p\right) = \left(F_p\right)^\gamma \frac{1}{\left[\left(F_p\right)^\gamma+\left(1-F_p\right)^\gamma\right]^{1/\gamma}} ~.
\ee
We note that no mechanistic motivation was given for the specific functional form. It simply ``looks a bit like the data.''
% 
The function $F_w^*\left(F_p\right)$ has only one free parameter, $\gamma$. For $\gamma=1$ it is the identity, and the CDFs coincide, $F_w^*\left(F_p\right)=F_p$. The function $F_w^*$ has the following property: any curvature moves the intersection with the diagonal away from the mid-point $1/2$. This means if the function is used to fit an inverse S, the fitting procedure itself introduces a shift of the intersection to the left. Because of this, we consider the key observation to be the inverse-S shape, whereas the shift to the left may be an artefact of the function chosen for the fit. We will see below that the curvature and shift correspond to different parts of a mechanistic explanation of the phenomenon.

\subsubsection{Mechanistic explanation of the inverse S}
Specifically, we now make explicit how the robust qualitative observation of the inverse-S shape in \fref{TK1992} emerges from assuming that the DM uses a larger scale in his model of the world than the DO. This can have numerous reasons, to which we will return in \secref{Reasons_for}. For now, suffice it to say that precaution is an obvious one: any uncertainty the DM wishes to include in his model in addition to what the DO includes will translate into a greater scale for the DM's distribution and therefore into an inverse-S shape for any unimodal (peaked) distribution when cumulative densities are compared.

We illustrate this with a Gaussian distribution.
Let's assume that a DO models an observable $x$ -- which will often be a future change in wealth -- as a Gaussian with location $\mu$ and variance $\sigma^2$. And let's further assume that a DM (for whatever reason, perhaps caution) models the same observable as a Gaussian with the same location, $\mu$, but with a greater scale, so that the variance is $\alpha^2\sigma^2$. The DM simply assumes a broader range -- $\alpha$ times greater -- of plausible values, left panel of \fref{probability_dists}.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{./figs/density_map.pdf}
\caption{{\bf Mapping PDFs.} Left: probability PDF (red), estimated by a DO; and decision-weight PDF (blue), estimated by a DM. The DO models $x$ with a best estimate for the scale (standard deviation) and assumes the true frequency distribution is the red line. The DM models $x$ with a greater scale (here 2 times greater, $\alpha=2$), and assumes the true frequency distribution is the blue line. Comparing the two curves, the DM appears to the DO as someone who over-estimates probabilities of low-probability events and underestimates probabilities of high-probability events, indicated by vertical arrows.
Right: the difference between assigned probabilities can also be expressed by directly plotting, for any value of $x$, the two different PDFs against one another. This corresponds to a non-linear distortion of the horizontal axis. The arrows on the left correspond to the same $x$-values as on the right. They therefore start and end at identical vertical positions as on the left. Because of the non-linear distortion of the horizontal axis, they are shifted to different locations horizontally.}
\flabel{probability_dists}
\end{figure}
%\subsection{Repetition and location uncertainty}
%
%Let's stay with the investment example, and again say annual logarithmic returns are Gaussian-distributed. Let's also assume (somewhat unrealistically) that we know the variance of these returns to be $\sigma_1^2$, with certainty.
%
%Finally, we assume that the mean of the returns fluctuates. Over time, we don't always draw from the same Gaussian, but sometimes from one with a higher mean and sometimes from one with a lower mean.
%We implement this as follows: first, generate a mean, $\gamma$, from a Gaussian distribution $\gamma \sim \ND(\mu, \sigma_2^2)$. Then draw a return from a Gaussian with this mean and variance $\sigma_1^2$, meaning $g\sim\ND(\gamma,\sigma_1^2)$. Over time, the total log return is just the sum of the annual log returns, and it is equivalent to the total log return that would be generated with a Gaussian with known mean and higher variance, $g\sim \ND(\mu,\sigma_1^2+\sigma_2^2)$.
%
%Considering a single round, a DO might well use the best estimate for $\gamma$, which is $\mu$, and the known variance $\sigma_1^2$, so that $g_p\sim \ND(\mu,\sigma_1^2)$. A DM, on the other hand, who has to live with the consequences of what happens in a long sequence realizations of $g$ would use $g_w\sim \ND(\mu,\sigma_1^2+\sigma_2^2)$. Again, the situations and incentives of the DO and the DM are different, leading them to use different models (the same models as in \Secref{Survival})
%
%\section{Analysis of the Gaussian case}
%The point of the previous two sub-sections was to illustrate two different ways in which uncertainty or fluctuations in parameters lead to behaviour by a DM that leads to greater decision weights for extreme events than the probabilities a DO might assign to them. In the first case, the DO uses the best estimate, rather than a conservative estimate of the variance. In the second case, the DO assumes no fluctuations in the mean return.
%
%Both DO models give $g_p\sim\ND(\mu,\sigma_1^2)$, whereas the DM will choose the model $g_w \sim \ND(\mu,\sigma_1^2+\sigma_2^2)$.

Generically, if the DM is using a greater scale in his model, then he is using higher decision weights for low-probability events, and (because of normalisation), lower decision weights for high-probability events than the corresponding model of the DO. We can express this by plotting, for any value of $x$, the decision weight {\it vs.} the probability observed at $x$, right panel of \fref{probability_dists}.

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.7\textwidth]{./figs/density_map.pdf}
%\caption{Decision weight density (used by a DM) vs. probability density (used by a DO) for the Gaussian model (blue), compared to the diagonal (black dashed) where DM and DO use the same parameters. For low probabilities, the decision weights are higher than the probabilities; for high probabilities they are lower.}
%\flabel{probability_weights}
%\end{figure}

In the Gaussian case we can write the distributions explicitly
\be \elabel{DecisionW}
	w(x)=\frac{1}{\sqrt{2\pi \alpha^2 \sigma^2}}\exp\left[\frac{-(x -\mu )^2}{2 (\alpha^2 \sigma^2)}\right]
\ee
and
\be
	p(x)=\frac{1}{\sqrt{2\pi \sigma^2}}\exp\left[\frac{-(x -\mu )^2}{2 \sigma^2}\right] ~,
\elabel{p}
\ee
% 
solve \eref{p} for $(x -\mu)^2$, substitute that in in \eref{DecisionW}, and obtain the following expression for decision weights directly as a function of probabilities
\be
w(p)= p^{\frac{1}{\alpha^2}} \frac{\left(2\pi\sigma_1^2\right)^{\frac{1-\alpha^2}{2\alpha^2}}}{\alpha} ~,
\elabel{w_of_p}
\ee
which is precisely what's plotted in the right panel of \fref{probability_dists}. As a sanity check, consider the shape of the $w(p)$ (blue curve, right panel \fref{probability_dists}): for a given value of $\alpha$, it is just a power law in $p$ with some pre-factor that ensures normalization. $\alpha>1$ means that the DM uses a greater standard deviation than the DO. In this case, the exponent of $p$ satisfies $\frac{1}{\alpha^2}<1$, and the blue curve is above the diagonal for small arguments and below it for large arguments.

%If we set the ratio of the DM's squared scale over the sum of the indivudal squared scales to $\alpha = \frac{\sigma_1^2}{\left(\sigma_1^2 + \sigma_2^2\right)}$, we can express \eref{w_of_p} as
%\be
%	w(p) =  \frac{p^{\frac{1}{\alpha^2}}}{\alpha} \sqrt{2\pi \sigma_1^2}^{\frac{1-\alpha^2}{\alpha^2}} . 
%\ee 
%
%\MK{Is there an interpretation to $\alpha$ or is it just a ratio? It's not the ratio of variances \ldots $\frac{\sigma^2}{(\sigma_1+\sigma_2)}^2$, maybe simply the ratio of the variance and the sum of the individual variances}
%

Alternatively, we can express the difference between models by plotting the CDFs $F_w$ and $F_p$. We do this in \fref{TwoCDFs}, revealing the origin of the inverted S as the DM assuming a greater scale.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./figs/GaussianFvsXtoWvsP.pdf}
\caption{{\bf Mapping CDFs.} 
Left: The DO assumes the observable $X$ follows Gaussian distribution $X \sim \ND(0,1)$, which results in the red CDF of the standard normal, $F_p(x) = \Phi_{0,1}(x)$. The DM is more cautious, in his model the same observable $X$ follows a wider Gaussian distribution, $X \sim \ND(0,3)$ depicted by $F_w$ (blue). 
% 
Following the vertical arrows (left to right), we see that for low values of the event probability $x$ the DM's CDF is larger than the DO's CDF, $F_p(x) < F_w(x)$; the curves coincide at 0.5 because no difference in location is assumed; necessarily for large values of the event probability $x$ the DM's CDF must be lower than the DO's.
Right: the same CDFs as on the left but now plotted not against $x$ but against the CDF $F_p$. Trivially, the CDF $F_p$ plotted against itself is the diagonal; the CDF $F_w$ now displays the generic inverse-S shape known from prospect theory. The arrows start and end at the same vertical values as on the left. Because the horizontal axis is has been non-linearly stretched (as the argument changed from $x$ to $F_p$), their horizontal locations are shifted.
}
\flabel{TwoCDFs}
\end{figure}

\FloatBarrier
\subsection{A mismatch between both scales and locations\seclabel{A_mismatch}}
In \fref{CDF_weights} we explore what happens if both the scales and the locations of the DO's and DM's models differ. Visually, this produces an excellent fit to empirical data, to which we will return in \secref{Fitting_the}.
 \begin{figure}
 \centering
 \includegraphics[width=1.0\textwidth]{./figs/Gauss_scale_location_both_KT.pdf}
 \caption{{\bf Decision weight CDFs used by a DM vs. probability CDFs used by a DO, Gaussian distribution}.\\ 
 Top left: Difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 1.64 (broader than DO).\\ 
 Top right: Difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.\\
 Bottom left: Differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.64 (broader than DO).\\ 
 Bottom right: Fit to observations reported by \cite{TverskyKahneman1992}. This is \eref{correspondence} with $\gamma=0.65$.
Note the similarity to bottom left.}
 \flabel{CDF_weights}
 \end{figure}

%% same figure using subfigure and reps. captions 
%\begin{figure}
%\begin{center}
%	\subcaptionbox{Gaussian distribution, difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 2.7 (broader than DO).\flabel{subfig:GaussLocation}}
%	[.45\textwidth]{\includegraphics[width=.45\linewidth]{./figs/Gauss_location.pdf}}
%	\subcaptionbox{Gaussian distribution, difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.\flabel{subfig:GaussScale}}
%	[.45\textwidth]{\includegraphics[width=.45\linewidth]{./figs/Gauss_scale.pdf}}
%	\\
%	\subcaptionbox{Gaussian distribution, differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 2.7 (broader than DO).\flabel{subfig:GaussLocScale}}
%	[.45\textwidth]{\includegraphics[width=.45\linewidth]{./figs/Gauss_location_and_scale.pdf}}
%%
%	\subcaptionbox{The observations by \cite{TverskyKahneman1992} are consistent with a DM assuming a scale and location in real-world decisions that differ from those assumed by the DO.\flabel{subfig:PW_TK}}
%	[.45\textwidth]{\includegraphics[width=.45\linewidth]{./figs/KT1992.pdf}}
%% 	\vspace{1em}
%\caption{{\bf Effects of differences in location and scale for the decision weight CDFs used by a DM vs. probability CDFs used by a DO.}\\
%\MK{Some more summarising text?}
%% Top left) Gaussian distribution, difference in scale. DO assumes location 0, scale 1; DM assumes location 0, scale 2.7 (broader than DO).\\ 
%% Top right) Gaussian distribution, difference in location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 1.
%% Bottom left) Gaussian distribution, differences in scale and location. DO assumes location 0, scale 1; DM assumes location 0.18 (bigger than DO), scale 2.7 (broader than DO).\\ 
%% Bottom right) Fit to observations reported by \cite{TverskyKahneman1992}. This is \eref{correspondence} with $\gamma=0.65$.
%% The observations by \cite{TverskyKahneman1992} are consistent with a DM assuming a scale and location in real-world decisions that differ from those assumed by the DO.
%\flabel{CDF_weights}
%\MK{Double check parameter values with py source code}
%}
%\end{center}
%\end{figure}

\FloatBarrier
\subsection{Different shapes, and fat-tailed distributions\seclabel{Different_shapes}}
Numerically, our procedure can be applied to arbitrary distributions: 
\begin{enumerate}
\item
construct a list of values for the CDF assumed by the DO, $F_p(x)$.
\item
construct a list of values for the CDF assumed by the DM, $F_w(x)$.
\item
plot $F_w(x)$ vs $F_p(x)$.
\end{enumerate}
Of course, the DM could even assume a distribution whose shape differs from that of the DO's distribution. An infinity of combinations of assumed distributions can be explored. The inverse S arises whenever a DM assumes a greater scale for a unimodal distribution. To illustrate the generality of the procedure, in \fref{fat_tailed_CDF} we carry it out for a (power-law tailed) Student's-t distribution, where DO and DM use different shape parameters and different locations. The result is qualitatively similar to the bottom right panel of \fref{TwoCDFs}, corresponding to \eref{correspondence}. 

A difference in assumed scales and locations is sufficient to reproduce the observations. This suggests a different nomenclature and a conceptual clarification. The inverse-S curve does not mean that ``probabilities are re-weighted'' -- it just means that experimenters and their subjects have different views about what might be an appropriate response to a situation. 

On 28 February 2020, Cass \citet{Sunstein2020}, a behavioral economist and former United States Administrator of the Office of Information and Regulatory Affairs, diagnosed that people's concern about a potential coronavirus outbreak in the US was attributable to an extreme case of probability weighting -- they neglected the fact, supposedly, that such an event had a low probability. When the piece was published, many commented that it seemed quite reasonable to them to take precautions, and Sunstein himself may have underestimated the severity of what lay ahead. One month later the US became the epicentre of the global coronavirus pandemic.
The episode illustrates that an inverted S-curve is a neutral indicator of a difference in opinion. It says nothing about who is right and who is wrong.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{./figs/Student-t.pdf}
\caption{Probability weighting for Student-t distributions, where the DM uses a different shape parameter (1) and a different location parameter (0) from those of the DO (2 and 0.2, respectively).}
\flabel{fat_tailed_CDF}
\end{figure}

%We denote these by $F_p=\int_{-\infty}^x p(s) ds$ (for the DO) and by $F_w=\int_{-\infty}^x w(s) ds$ (for the DM). 
%We will use the terms scale and location, rather than mean and standard deviation, to emphasise the generality of our arguments: 

%To clarify this nomenclature: starting with the standard form of a distribution $P(x)$ (of scale 1 and location 0), for example $P_{\text{standard}}(x)\sim \ND(0,1)$, the corresponding distributions of scale $\sigma$ and location $\mu$ are obtained by transforming $x$. With $y=\frac{x-\mu}{\sigma}$, we have $P_{\text{standard}}(y)=\ND(\mu, \sigma^2)$.

\FloatBarrier
\section{Reasons for different models\seclabel{Reasons_for}}
``Probability weighting'' as a term is suggestive of a detrimental cognitive bias. We caution against this interpretation. At the very least it must be borne in mind that it is unclear who suffers from the bias: experimenters or test subjects? It is interesting, however, that the difference in opinion tends to go in the same direction. In more neutral language, DMs tend to assume a greater range of plausible outcomes than DOs. Why might this be? Two conditions must be satisfied. First, there must be a reason for frequent disagreement about probabilities used in a model; second, there must be a reason for such disagreement to be consistent: there must be a relevant systematic difference between the DO and the DM. 

The first condition is satisfied because the word ``probability'' is commonly interpreted in different ways. Even once one has settled on a definition, numerical values for probabilities are still difficult to estimate from real-world observations (see \secref{tricky}).

The second condition is satisfied as follows. A DO has to build a formal model, and will include a number of sources of uncertainty in it, but often not all sources. A DM instead has to make decisions in the real world, and rules of thumb like the precautionary principle (better err on the side of caution) will often lead to additional assumed uncertainty. For example, the DO may know the true probabilities of some gamble in an experiment; the DM may in addition have doubts about the DO's sincerity, or about his (the DM's) understanding of the rules of the game. We will return to this in \secref{condition2}.

\subsection{``Probability'' can mean different things \seclabel{tricky}}

Many thousands of pages have been written about the meaning of probability. We will not attempt a summary of the philosophical debate and instead focus on a few relevant points.
%systematically different incentives from a real-world DM. The DO tends to have an incentive to estimate, or indeed state, probabilities as their most likely value or ``true'' value (if the DO can control the probabilities, \eg in an experiment); the DM simply strives to behave optimally under uncertainty, and that uncertainty often includes uncertainty about the probabilities or model parameters.

%The conundrum surrounding the observed phenomenon of the characteristic inverse-S-shaped probability weighting vanishes if probability is understood as individual ignorance, \ie somebody's uncertainty about an observable.

\paragraph{Frequency-in-an-ensemble interpretation of probability}
Consider the simple probabilistic statement: ``the probability of rain here tomorrow is 70\%.'' Tomorrow only happens once, so one might ask: in 70\% of what will it rain? The technical answer to this question is often: rain happens in 70\% of the members of an ensemble of computer simulations, run by a weather service, of what may happen tomorrow. So one interpretation of ``probability'' is ``relative frequency in a hypothetical ensemble of possible futures.'' 

It is thus a statement about a model. How exactly it is linked to physical reality is not completely clear. 

%Sometimes ensembles are real, for instance, when we say the probability of having a car accident is 1\% per 10000 km driven -- that's a summary of statistics collected over a large ensemble of cars. In this case, it's a real ensemble that existed in the past, not an imagined one in the future. 

\paragraph{Frequency-over-time interpretation of probability}
In some situations, the statement ``70\% chance of rain tomorrow'' refers to the relative frequency over time. Before the advent of computer models in weather forecasting, people used to compare recent measurements (of wind and pressure today, say) to measurements further in the past -- weeks, months, years earlier, that were similar and where one had reason to believe that what had happened 1 day later would be similar to what will happen tomorrow.

Rather than a statement about outcomes of an in-silico model, the statement may thus be a summary of real-world observations over a long time.

\paragraph{Degree-of-belief interpretation of probability}
No matter how ``probability'' relates to a frequentist physical statement (whether with respect to an ensemble of simultaneously possible futures or to a sequence of actual past futures), it also corresponds to a mental state of believing something with a degree of conviction: ``I'm 90\% sure I left my wallet in that taxi.''

For our purpose it suffices to say that there's no guarantee that a probabilistic statement will be interpreted by the listener (the DM) as it was intended by whoever made the statement (the DO).

\subsection{Consistent differences between DO and DM \seclabel{condition2}}

\subsubsection*{Estimation errors for probabilities}
Let's imagine the DO and DM have agreed explicitly on an interpretation of the word ``probability.'' Say they agree on the relative frequency of an event in an infinitely long sequence of observations. Suppose this has a value $p$. In reality the number of observations made is finite so, to estimate the probability so defined, we count: if the event occurred in $n$ out of $T$ observations, then we estimate $p$ as $\hat{p} \equiv \frac{n}{T}$.

Statistical inference -- the business of estimating parameter values, like $p$, and their uncertainties from samples -- is not the focus of this paper. Therefore, we will stick to a very simple example. At each of the observations, the event occurs with fixed probability $p$, independently of all other observations. The relative frequency, $\hat{p}$, is a binomial random variable with mean $p$ and standard deviation $\sqrt{\frac{p(1-p)}{T}}$. Of course, we don't know $p$ -- we are trying to estimate it -- but we can substitute the observed $\hat{p}$ to estimate the standard error, $\sqrt{\frac{\hat{p}(1-\hat{p})}{T}}$, in a measurement of $\hat{p}$. The relative error is $\sqrt{\frac{1-\hat{p}}{\hat{p}T}}$


%In the simplest case (and we rarely consider anything more complicated), we model the arrival of events as a Poisson process, where the standard error in the count of an event famously goes as $\sqrt{n_i}$. The standard error in the probability of an event is therefore $\frac{\sqrt{n_i}}{T}$, and the relative error is $\frac{1}{\sqrt{n_i}}$.

Note the $\frac{1}{\sqrt{\hat{p}}}$ dependence in the relative error: smaller probabilities have larger relative uncertainties, see \tref{errors}. We note that behaviourally, it will make little difference whether a DM assigns a 0.49 probability to an event or a 0.51 probability. It will make a large difference, however, whether a DM assigns a 0.0001 probability or a 0 probability.

\begin{table}[!htb]
%\ra{1.25}
%\small
\centering
\begin{tabular}{@{}lllll@{}}
\toprule[2pt]
\makecell{Asymptotic\\(true) probability} & \makecell{Most likely\\count} & \makecell{Standard error\\ in count} & \makecell{Standard error\\ in probability} &  \makecell{Relative error\\ in probability}\\
\midrule[2pt]
%.5 & 5000 & .01 & 2\%\\
%.1 & 1000& .003 & 3\%\\
%.01 & 100& .001 & 10\%\\
%.001 & 10& .0003& 30\%\\
%.0001 & 1& .0001 &100\%\\

0.5 & 5000 & 50.0 & 0.0050 & 0.0100\\
0.1 & 1000 & 30.0 & 0.0030 & 0.0300\\
0.01 & 100 & 9.9 & 0.0010 & 0.0995\\
0.001 & 10 & 3.2 & 0.0003 & 0.3161\\
0.0001 & 1 & 1.0 & 0.0001 & 0.9999\\
\bottomrule[2pt]
\end{tabular}
\caption{This table assumes $T = 10000$ observed time intervals. To be read as follows (first line): for an event of true probability 0.5, the most likely count in 10000 trials is 5000. Assuming Poisson statistics, this comes with an estimation error of $\sqrt{5000}/5000=0.01$, which is 2\% of the true probability.}\tlabel{errors}
\end{table}

The most important message from this example is that errors in probability estimates behave differently for low probabilities than for high probabilities: absolute errors are smaller for low probabilities, but relative errors are larger for lower probabilities. 

Let's make one more assumption: DMs don't like surprises. In order to avoid surprises, the DM will incorporate the uncertainty in any probability estimates by assuming that the actual long-time relative frequency of an event is the best estimate plus two standard errors, so that
\be
w(x)=\frac{p(x)+2 \sqrt{\frac{p(x)(1-p(x))}{T}}}{\int_{-\infty}^{\infty}\left[p(s)+2 \sqrt{\frac{p(s)(1-p(s))}{T}}\right] ds},
\elabel{Poisson}
\ee
where the denominator ensures normalization, so that $w(x)$ is a proper probability density. \Fref{square_root_error} shows the resulting PDFs and the CDF mappings, for a Gaussian distribution and for a fat-tailed Student-t distribution. As one might expect, the effect is more pronounced for the fat-tailed distribution.
\begin{figure}[htb]
\centering
\includegraphics[width=0.9\textwidth]{./figs/square_root_error.pdf}
\caption{PDFs and inverse-S curves arising when the DO assumes a Gaussian (scale 1, location 0, top line) or a Student-t distribution (shape 2, location 0, bottom line), and the DM uses decision weights in accordance with \eref{Poisson}. For the fat-tailed Student-t distribution the difference between $p$ and $w$ is more pronounced.}
\flabel{square_root_error}
\end{figure}

\FloatBarrier
\subsubsection{Typical situations of DO and DM: ergodicity}
To recap: behavioral economics observes that DOs tend to assign lower weights to low-probability events than DMs.
While behavioral economics commonly assumes that the DM is wrong, we make no such judgement. In any decision problem, the aim of the decision must be taken into account, and that crucially depends on the situation of the individual. 

The two types of modellers (DO and DM) pursue different goals. Broadly, the DO tends to be a behavioral scientist without personal exposure to the success or failure of the DM (who tends to be a test subject or someone whose behavior is being observed in the wild). The DM, of course, has such exposure. Throughout the history of economics, it has been a common mistake, by DOs, to assume that DMs optimise what happens to them on average in an ensemble. To the DM what happens to the ensemble is usually not a primary concern -- instead, the concern of the DM is what happens to him over time. Not distinguishing between these two perspectives is only permissible if they lead to identical predictions, and that is only the case in ergodic situations \citep{Peters2019b}. 

It is now well known that the situation usually studied in decision theory is not ergodic in the following sense: DMs are usually observed making choices that affect their wealth, and wealth is usually modelled as a stochastic process that is not ergodic. The ensemble average of wealth does not behave like the time average of wealth.

The most striking example is the universally important case of noisy multiplicative growth -- universal because it is the fundamental process that drives evolution: noise generates the diversity of phenotypes necessary for evolution and multiplicative growth (self-reproduction) is how successful phenotypes spread their traits in a population. This process operates on amoeba, as it does on forms of institutions, and on investment strategies.
%\citep{PetersAdamou2015a}

The simplest model of noisy multiplicative growth is geometric Brownian motion, $dx=x(\mu dt+\sigma dW)$. The average over the full statistical ensemble (often studied by the DO) of geometric Brownian motion grows as $\exp(\mu t)$. The individual trajectory of geometric Brownian motion, on the other hand, grows in the long run as $\exp[(\mu-\frac{\sigma^2}{2})t]$.

In the DO's ensemble perspective, noise does not affect growth and is often deemed irrelevant. In the DM's time perspective, noise reduces growth. While a DO interested in the ensemble may get away with disregarding low-probability events, for the DM's success hedging against them is of crucial importance.

The difference between how these two perspectives evaluate the effects of noise (\ie of the probabilistic events) is qualitatively in line with the observed phenomena we set out to explain. The DM typically has large uncertainties, especially for low-probability events, and has an evolutionary incentive to err on the side of caution, \ie to behave as though low-probability (extreme) events had a higher probability than in the DO's model.


\section{Fitting the model to experimental results \seclabel{Fitting_the}}

Visually, looking at the figures and the level of noise in the data in \fref{TK1992}, one would conclude that Tversky and Kahneman's physically unmotivated fit of  $F^*_w$, \eref{correspondence}, resembles the data no better than our mechanistically constrained model.
This is particularly evident in the bottom two panels of \fref{CDF_weights}, which show that a Gaussian $w(x)$ whose scale and location differ from those of $p(x)$ reproduces the fitted functional shape of $F^*_w$.

For completeness and scientific hygiene, in the present section we fit Gaussian location ($\mu$) and scale ($\sigma$) parameters and Student-t shape parameters ($\nu$ and $\delta$) for $w(x)$ to experimental data from \cite{TverskyKahneman1992} (depicted in circles in \fref{TK1992}) and from \cite{TverskyFox1995}. In addition, we fit the function
%
\be
w(p)=\frac{\delta p^{\gamma}}{\delta p^{\gamma} + (1-p)^{\gamma}}\,,
\elabel{LattimoreFunction}
\ee
%
suggested by \cite{LattimoreBakerWitte1992} to parametrically describe probability weighting, and was commonly used since (see, \eg \cite{tversky1995risk}).

\Fref{TK_TF_fit} presents these results. We obtain very good fits to data for both Gaussian and Student-t distributions, as well as for \eref{LattimoreFunction}, in the two experiments. It is practically impossible to distinguish between the three fitted functions within standard errors. We conclude that our model fit the data well, and unlike \eref{LattimoreFunction}, the fitted functions are directly derived from a mechanistic model, and are not simply phenomenological.

\begin{figure}[htb]
\centering
\includegraphics[width=1.0\textwidth]{./figs/TK_TF_fit.pdf}
\caption{Model fitting to experimental data from \cite{TverskyKahneman1992} (left) and \cite{TverskyFox1995} (right).
Left) Gaussian model: $\mu=0.4\,\left(SE = 0.13\right)$, $\sigma=1.5\,\left(\pm0.2\right)$; Student's-t model: $\nu=0.39\,\left(\pm0.06\right)$, $\delta=0.29\,\left(\pm0.08\right)$; \cite{LattimoreBakerWitte1992}: $\delta=0.67\,\left(\pm0.07\right)$, $\gamma=0.59\,\left(\pm0.06\right)$. Right) Gaussian model: $\mu=0.23\,\left(\pm0.03\right)$, $\sigma=1.4\,\left(\pm0.06\right)$; Student's-t model: $\nu=0.47\,\left(\pm0.05\right)$, $\delta=0.17\,\left(\pm0.03\right)$; \cite{LattimoreBakerWitte1992}: $\delta=0.77\,\left(\pm0.02\right)$, $\gamma=0.69\,\left(\pm0.02\right)$. Shaded areas (noticeable only in the case of \cite{TverskyKahneman1992}) indicate one standard error in the fitted parameter values. The fit was done by implementing the method of least squares with the Nelder-Mead algorithm \cite{NelderMead1965}, and the standard errors were obtained by bootstrapping.}
\flabel{TK_TF_fit}
\end{figure}

\FloatBarrier
\section{Conclusion/Summary}
\OP{We could move some discussion bits here -- like the incredible Sunstein story.}
\MK{Discuss the general action of change of measure in relation to Girsanov and risk-neutral prob measure $\mathbb{Q}$ in Math Finance?}
\OP{Let's keep this paper focused on one point only.}
%\MK{It has almost become custom to end with a Kacelnik quote}



% BibTeX users please use one of
\bibliographystyle{apalike}
\bibliography{../LML_bibliography/bibliography}   % name your BibTeX data base
\end{document}

